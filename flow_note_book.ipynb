{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Use CUDA, if you have no GPU, set 'cuda' to 'cpu'**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "torch.set_default_device(\"cuda\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data from local files.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "def load_data():\n",
        "    train_data = []\n",
        "    train_label = []\n",
        "    test_data = []\n",
        "    test_label = []\n",
        "    with open(\"./cifar-10-batches-py/data_batch_1\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        train_data.extend(dict[\"data\"])\n",
        "        train_label.extend(dict[\"labels\"])\n",
        "    with open(\"./cifar-10-batches-py/data_batch_2\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        train_data.extend(dict[\"data\"])\n",
        "        train_label.extend(dict[\"labels\"])\n",
        "    with open(\"./cifar-10-batches-py/data_batch_3\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        train_data.extend(dict[\"data\"])\n",
        "        train_label.extend(dict[\"labels\"])\n",
        "    with open(\"./cifar-10-batches-py/data_batch_4\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        train_data.extend(dict[\"data\"])\n",
        "        train_label.extend(dict[\"labels\"])\n",
        "    with open(\"./cifar-10-batches-py/data_batch_5\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        train_data.extend(dict[\"data\"])\n",
        "        train_label.extend(dict[\"labels\"])\n",
        "    with open(\"./cifar-10-batches-py/test_batch\", 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "        test_data.extend(dict[\"data\"])\n",
        "        test_label.extend(dict[\"labels\"])\n",
        "    return (torch.tensor(train_data)/255.0, \n",
        "    F.one_hot(torch.tensor(train_label), num_classes=10), \n",
        "    torch.tensor(test_data)/255.0, \n",
        "    F.one_hot(torch.tensor(test_label), num_classes=10))\n",
        "    return (train_data,train_label,test_data,test_label)\n",
        "\n",
        "train_data,train_label,test_data,test_label=load_data()\n",
        " "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/pyt310/lib/python3.10/site-packages/torch/utils/_device.py:104: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  return func(*args, **kwargs)\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1740730003167
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the label to float**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_label = train_label *1.0\n",
        "test_label = test_label *1.0"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1740730005281
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the shape of data**\n",
        "\n",
        "**Images are 32x32 RGB images**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.reshape((50000,32,32,3))\n",
        "test_data = test_data.reshape((10000,32,32,3))\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1740730007017
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Expert module, this is the E of MoE**\n",
        "\n",
        "**Normally, the model structure does not need to be changed.**\n",
        "\n",
        "**If you want to change something, please ensure that the shapes are suitable.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):  \n",
        "    def __init__(self, output_dim):  \n",
        "        super(Expert, self).__init__()  \n",
        "        self.conv1 = nn.Conv2d(32, 64, (2,1))\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, (2,1))\n",
        "        self.fc1 = nn.Linear(1792, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, output_dim)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "          \n",
        "    def forward(self, x):  \n",
        "        x= F.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
        "        x = x.view(-1, 1792)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1740730009003
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Sparse Gate module, it is used to select which expert should go to work.**\n",
        "\n",
        "**In this situation, I created a same model structure with Expert.**\n",
        "\n",
        "**But I think Sparse Gate should output a sigmoid result in order to make meaning clearly.**\n",
        "\n",
        "**In the forward(), I made some experts' score to 0 if the calcuated score was lower than given threshold**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseGate(nn.Module):  \n",
        "    def __init__(self,num_experts, threshold=0.5):  \n",
        "        super(SparseGate, self).__init__()  \n",
        "        self.num_experts = num_experts\n",
        "        self.conv1 = nn.Conv2d(32, 64, (2,1))\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, (2,1))\n",
        "        self.fc1 = nn.Linear(1792, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_experts)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.threshold = threshold  \n",
        "          \n",
        "    def forward(self, x):  \n",
        "        x= F.leaky_relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
        "        x = x.view(-1, 1792)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        scores = torch.sigmoid(x)   \n",
        "        # get k of top k\n",
        "        k = self.num_experts * self.threshold\n",
        "        topk,_ = torch.topk(scores, int(k))\n",
        "        mask = []\n",
        "        for index in range(self.num_experts):\n",
        "            #if scores[0][index] > topk.min() or scores[0][index] == topk.min():\n",
        "            if scores[0][index] > self.threshold:\n",
        "                mask.append(scores[0][index])\n",
        "            else:\n",
        "                mask.append(0.0)\n",
        "        mask =torch.tensor(mask)\n",
        "        # mask all zero scores\n",
        "        return mask"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1740730011238
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This module is a fake module.**\n",
        "\n",
        "**It is useful even be fake, I use it to replace the Real Expert with 0 score from Sparse Gate module.**\n",
        "\n",
        "**It will never joint to the calculation.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpertMask(nn.Module): \n",
        "    def __init__(self, output_dim):  \n",
        "        super(ExpertMask, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.fl = nn.Flatten()\n",
        "        self.mask = nn.Linear(1024*3, output_dim)\n",
        "    def forward(self, x):  \n",
        "        v = self.mask(self.fl(x))*0.0\n",
        "        return v"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1740730013638
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the main dispatcher module of MoE.**\n",
        "\n",
        "**It calls Sparse Gate to calculate experts's score. **\n",
        "\n",
        "**It calls each activated expert to work.**\n",
        "\n",
        "**It calculates the final result.**\n",
        "\n",
        "**This is a multiple tags task. So the result should be calculated with Sigmoid.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoE(nn.Module):  \n",
        "    def __init__(self,  output_dim, num_experts, threshold=0.5):  \n",
        "        super(MoE, self).__init__()  \n",
        "        self.experts = nn.ModuleList([Expert( output_dim) for _ in range(num_experts)])  \n",
        "        self.gate = SparseGate(num_experts, threshold)  \n",
        "        self.num_experts = num_experts\n",
        "        self.output_dim =output_dim\n",
        "          \n",
        "    def human_setup_compute(self, x, human_set_gate_factors = None):\n",
        "        if human_set_gate_factors is not None:\n",
        "            expert_scores = human_set_gate_factors\n",
        "        activated_experts  = []\n",
        "        for expert_index in range(self.num_experts):\n",
        "            if expert_scores[expert_index] > torch.tensor(0.0):\n",
        "                activated_experts.append(self.experts[expert_index])\n",
        "            else:\n",
        "                activated_experts.append(ExpertMask(self.output_dim))\n",
        "        expert_outputs = torch.stack([expert(x) if isinstance(expert, Expert) else torch.zeros((x.shape[0],self.output_dim)) for expert in activated_experts], dim=1) \n",
        "        \n",
        "        gate_outputs = expert_scores.unsqueeze(-1) \n",
        "        \n",
        "        final_output = torch.sum(gate_outputs * expert_outputs, dim=1) \n",
        "        final_output = F.sigmoid(final_output)\n",
        "        #print(final_output)\n",
        "        return final_output  \n",
        "\n",
        "    def forward(self, x):  \n",
        "        expert_scores = self.gate(x)  \n",
        "        activated_experts  = []\n",
        "        for expert_index in range(self.num_experts):\n",
        "            if expert_scores[expert_index] > torch.tensor(0.0):\n",
        "                activated_experts.append(self.experts[expert_index])\n",
        "            else:\n",
        "                activated_experts.append(ExpertMask(self.output_dim))\n",
        "        expert_outputs = torch.stack([expert(x) if isinstance(expert, Expert) else torch.zeros((x.shape[0],self.output_dim)) for expert in activated_experts], dim=1)\n",
        "        gate_outputs = expert_scores.unsqueeze(-1) \n",
        "        \n",
        "        final_output = torch.sum(gate_outputs * expert_outputs, dim=1) \n",
        "        final_output = F.sigmoid(final_output)\n",
        "        #print(final_output)\n",
        "        return final_output  "
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1740733079037
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I defined a loss function.**\n",
        "\n",
        "**But it is useless now. I changed to BCELoss in the training work.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x,y):\n",
        "        return torch.sum(torch.abs(x-y))"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1740730018776
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**You can set number of experts, it will be perfect if you set a lot of experts, but the whole model will be heavy with more parameters.**\n",
        "\n",
        "**batch_size, you know, it can improve the training, faster and more accurate**\n",
        "\n",
        "**threshold means the activation condition. if the sparse gate give a score less than it, the target expert will be replaced by ExpertMask, that fake Expert module**\n",
        "\n",
        "**Learning Rate is set to 0.001, if you changed the modules before, but training convergence is too slow, you can change it.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_data: torch.Tensor, train_label: torch.Tensor, test_data: torch.Tensor, test_label: torch.Tensor):\n",
        "    output_dim = 10  \n",
        "    num_experts =20\n",
        "    threshold = 0.5\n",
        "    batch_size = 100\n",
        "    #model = Expert(output_dim)\n",
        "    model = MoE(output_dim, num_experts, threshold)  \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_function = nn.BCELoss()\n",
        "    loss_sum = 0\n",
        "    for epoch in range(10):\n",
        "        for step in range(0, 50000, batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            step_result = model(train_data[step:step+batch_size])\n",
        "            loss = loss_function(step_result, train_label[step:step+batch_size])\n",
        "            loss_sum = loss_sum+ loss.item()\n",
        "            loss.backward() \n",
        "            optimizer.step() \n",
        "            #print([x.grad for x in optimizer.param_groups[0][\"params\"]])\n",
        "            # for params in model.parameters():\n",
        "            #     print(f\"G: {params.grad}\")\n",
        "        print(f'E:{epoch} {loss_sum/500}')\n",
        "        loss_sum = 0\n",
        "    return model\n",
        "                 \n"
      ],
      "outputs": [],
      "execution_count": 81,
      "metadata": {
        "gather": {
          "logged": 1740734382273
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start to train**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train(train_data, train_label, test_data, test_label)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "E:0 0.2285371019244194\nE:1 0.18469052982330322\nE:2 0.16142360980808734\nE:3 0.1393040187060833\nE:4 0.1167462460398674\nE:5 0.09433456543833017\nE:6 0.07594361340999603\nE:7 0.06724514252319932\nE:8 0.060273028373718264\nE:9 0.05009632034227252\n"
        }
      ],
      "execution_count": 82,
      "metadata": {
        "gather": {
          "logged": 1740734518147
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the index of testing picture.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index =78"
      ],
      "outputs": [],
      "execution_count": 83,
      "metadata": {
        "gather": {
          "logged": 1740734561615
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**try the model**\n",
        "\n",
        "**First line: Sparse Gate result.**\n",
        "\n",
        "**Second line: Prediction tags of the model**\n",
        "\n",
        "**Third line: Actual tags**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    print(trained_model.gate(test_data[index].reshape((1,32,32,3))))\n",
        "    print(trained_model(test_data[index].reshape((1,32,32,3))))\n",
        "    print(test_label[index])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([0.0000, 0.0000, 0.5315, 0.0000, 0.5111, 0.0000, 0.0000, 0.0000, 0.5145,\n        0.5742, 0.5114, 0.0000, 0.0000, 0.0000, 0.0000, 0.5085, 0.5149, 0.5038,\n        0.0000, 0.5587], device='cuda:0')\ntensor([[8.6667e-04, 3.6607e-01, 1.9934e-02, 7.4488e-01, 2.2469e-04, 4.7849e-05,\n         3.3677e-03, 3.7516e-05, 2.6871e-02, 6.3244e-01]], device='cuda:0')\ntensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
        }
      ],
      "execution_count": 84,
      "metadata": {
        "gather": {
          "logged": 1740734562211
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can skip the Sparse Gate module and set the scores manually.**\n",
        "\n",
        "**You may see that:**\n",
        "\n",
        "\n",
        "**1. The accuracy is not decrease even make some more expert's score to zero.**\n",
        "\n",
        "**2. If you set some expert's score to higher, the accuracy will be higher or lower.**\n",
        "\n",
        "**So, you can see that, The Sparse Gate Module should be trained when the expert modules being frozen.**\n",
        "\n",
        "**This is the MoE training process: a) Whole model. b) Sparse Gate when freeze experts. c)Experts when freeze Sparse Gate**\n",
        "\n",
        "**Maybe the whole training will include a lot of iterations(a group of a b c)**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    ll = trained_model.human_setup_compute(test_data[index:index+1], \n",
        "    torch.tensor([0.0000, 0.0000, 0.5315, 0.0000, 0.5111, 0.0000, 0.0000, 0.0000, 0.5145,\n",
        "        0.5742, 0.5114, 0.0000, 0.0000, 0.0000, 0.0000, 0.5085, 0.9149, 0.9038,\n",
        "        0.0000, 0.2587]))\n",
        "    print(ll)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[4.5552e-04, 3.7660e-01, 4.0578e-02, 9.6198e-01, 3.8759e-04, 2.2229e-05,\n         5.4764e-03, 3.8834e-05, 2.1863e-02, 5.6670e-01]], device='cuda:0')\n"
        }
      ],
      "execution_count": 96,
      "metadata": {
        "gather": {
          "logged": 1740734724550
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "mykernel",
      "language": "python",
      "display_name": "My Kernel"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "mykernel"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}